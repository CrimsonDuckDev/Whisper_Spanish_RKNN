{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b7d7d63",
   "metadata": {},
   "source": [
    "# Introducción\n",
    "\n",
    "En este notebook, aprenderemos cómo descargar un modelo preentrenado de OpenAI Whisper desde Hugging Face y adaptarlo al formato RKNN para ejecutarlo en un chip Rockchip RK3588. Este proceso es útil para aprovechar las capacidades de inferencia de hardware del RK3588 al trabajar con modelos de procesamiento de lenguaje natural y reconocimiento de voz.\n",
    "\n",
    "## Requisitos\n",
    "\n",
    "Este notebook ha sido probado en Python 3.10.16 y utiliza la herramienta RKNNToolkit v2.3.0. Asegúrate de instalar las dependencias necesarias antes de continuar.\n",
    "\n",
    "### Instalación de dependencias\n",
    "\n",
    "1. Instala las dependencias requeridas para RKNNToolkit v2.3.0 desde el siguiente archivo de requisitos:\n",
    "    ```\n",
    "    https://github.com/airockchip/rknn-toolkit2/blob/v2.3.0/rknn-toolkit2/packages/x86_64/requirements_cp310-2.3.0.txt\n",
    "    ```\n",
    "    Puedes instalarlas ejecutando:\n",
    "    ```bash\n",
    "    pip install -r requirements_cp310-2.3.0.txt\n",
    "    ```\n",
    "\n",
    "2. Instala el paquete `rknn_toolkit2` correspondiente:\n",
    "    ```\n",
    "    https://github.com/airockchip/rknn-toolkit2/blob/v2.3.0/rknn-toolkit2/packages/x86_64/rknn_toolkit2-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
    "    ```\n",
    "    Puedes instalarlo ejecutando:\n",
    "    ```bash\n",
    "    pip install rknn_toolkit2-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
    "    ```\n",
    "\n",
    "3. Instala las siguientes bibliotecas adicionales necesarias para trabajar con el modelo Whisper:\n",
    "    ```bash\n",
    "    pip install openai-whisper librosa onnxsim soundfile\n",
    "    ```\n",
    "\n",
    "## Paso 1. Descargar el modelo convertido en formato ONNX\n",
    "\n",
    "Una vez que hayas instalado todas las dependencias, estarás listo para continuar con el proceso de descarga, conversión y optimización del modelo Whisper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36c526d",
   "metadata": {},
   "source": [
    "Imports y definiendo valores de configuración para el proceso de descarga del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d5f348b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manuel/.miniconda3/envs/rknn/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import whisper\n",
    "from whisper import audio as whisper_audio # Específicamente para mel_filters\n",
    "import onnx\n",
    "from onnxsim import simplify\n",
    "from onnx import shape_inference\n",
    "import torch\n",
    "import numpy as np # Necesario para guardar los filtros mel con formato\n",
    "import argparse\n",
    "import warnings\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration, WhisperConfig\n",
    "\n",
    "# --- Configuraciones Iniciales ---\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) # Ignorar warnings comunes de HuggingFace/Torch\n",
    "\n",
    "# Directorio para guardar modelos, vocabulario, filtros, etc.\n",
    "MODEL_SAVE_DIR = \"./model\"\n",
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True) # Crear directorio si no existe\n",
    "\n",
    "DEFAULT_MODEL_NAME = \"rjac/whisper-tiny-spanish\" # Modelo Whisper a usar\n",
    "DEVICE = torch.device(\"cpu\") # Dispositivo para PyTorch (exportación se hace en CPU)\n",
    "FIXED_BATCH_SIZE = 1 # Tamaño de batch fijo para exportación ONNX\n",
    "FIXED_NUM_FRAMES = 3000 # Número fijo de frames Mel (correspondiente a 30s de audio a 16kHz)\n",
    "N_MELS_DEFAULT = 80 # Número de bandas Mel por defecto (se confirmará con el modelo)\n",
    "N_FFT = 400 # Tamaño de la FFT\n",
    "FIXED_DECODER_INPUT_LENGTH = 12\n",
    "\n",
    "mel_filters_path = None # Ruta para guardar los filtros Mel\n",
    "vocab_path = None # Ruta para guardar el vocabulario\n",
    "onnx_encoder_path = None # Ruta para guardar el modelo ONNX del encoder\n",
    "onnx_decoder_path = None # Ruta para guardar el modelo ONNX del decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc6d964",
   "metadata": {},
   "source": [
    "Funciones utilitarias para el proceso de descarga del modelo:\n",
    "\n",
    " 1.  **Carga el modelo y procesador** de Hugging Face (`rjac/whisper-tiny-spanish`).\n",
    " 2.  **Extrae los filtros Mel** específicos usados por Whisper y los guarda en un archivo `.txt`. Estos filtros son necesarios para calcular correctamente el espectrograma Mel durante la inferencia.\n",
    " 3.  **Extrae el vocabulario** del tokenizador, incluyendo tokens especiales y de timestamp (generándolos si es necesario), y lo guarda en `vocab_es.txt`. Este archivo mapea IDs de tokens a sus representaciones textuales.\n",
    " 4.  **Prepara datos de ejemplo** con las dimensiones correctas (batch=1, n_mels, frames=3000) para la exportación.\n",
    " 5.  **Exporta el Encoder** del modelo a formato ONNX (`whisper_encoder_*.onnx`).\n",
    " 6.  **Exporta el Decoder** (incluyendo la capa de proyección final) a formato ONNX (`whisper_decoder_*.onnx`).\n",
    " 7.  **Simplifica y añade información de forma** a los modelos ONNX generados para optimizarlos y mejorar la compatibilidad.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c96b665",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_model(model_name):\n",
    "    \"\"\" Carga el modelo y procesador de Hugging Face. \"\"\"\n",
    "    print(f\"Cargando modelo y procesador desde Hugging Face: {model_name}\")\n",
    "    if whisper is None: # Verificar si la importación de whisper funcionó\n",
    "        raise ImportError(\"La biblioteca 'whisper' no está instalada o no se pudo importar.\")\n",
    "    try:\n",
    "        processor = WhisperProcessor.from_pretrained(model_name)\n",
    "        model = WhisperForConditionalGeneration.from_pretrained(model_name).to(DEVICE).eval()\n",
    "        config = model.config\n",
    "        print(f\"Modelo {model_name}, Procesador y Configuración cargados.\")\n",
    "\n",
    "        # Extraer dimensiones clave (usando nombres consistentes con inferencia)\n",
    "        global N_MELS # Usar global para actualizar la constante si es necesario\n",
    "        N_MELS = processor.feature_extractor.feature_size\n",
    "        D_MODEL = config.d_model\n",
    "        VOCAB_SIZE = config.vocab_size\n",
    "        # Longitud de secuencia de salida del encoder (depende de los frames de entrada y la reducción del modelo)\n",
    "        # Para Whisper estándar con 3000 frames (30s), la salida suele ser 1500 tokens.\n",
    "        ENCODER_SEQUENCE_LENGTH = FIXED_NUM_FRAMES // 2\n",
    "        MAX_TARGET_POSITIONS = config.max_target_positions # Límite de tokens del decoder\n",
    "\n",
    "        print(\"\\n--- Dimensiones Clave del Modelo ---\")\n",
    "        print(f\"  Nombre del Modelo: {model_name}\")\n",
    "        print(f\"  Batch Size (Fijo para Exportación): {FIXED_BATCH_SIZE}\")\n",
    "        print(f\"  Bandas Mel (N_MELS): {N_MELS}\")\n",
    "        print(f\"  Frames Mel Fijos (Entrada Encoder): {FIXED_NUM_FRAMES}\")\n",
    "        print(f\"  Longitud Secuencia Salida Encoder: {ENCODER_SEQUENCE_LENGTH}\")\n",
    "        print(f\"  Tamaño Oculto (d_model): {D_MODEL}\")\n",
    "        print(f\"  Tamaño Vocabulario: {VOCAB_SIZE}\")\n",
    "        print(f\"  Máx Tokens Decoder: {MAX_TARGET_POSITIONS}\")\n",
    "        print(\"-----------------------------------\\n\")\n",
    "\n",
    "        # Actualizar N_MELS_DEFAULT por si acaso se usa más adelante\n",
    "        global N_MELS_DEFAULT\n",
    "        N_MELS_DEFAULT = N_MELS\n",
    "\n",
    "        return model, processor, config\n",
    "    except Exception as e:\n",
    "        print(f\"Error cargando el modelo/procesador '{model_name}': {e}\")\n",
    "        print(\"Verifica que el nombre del modelo es correcto y tienes conexión a internet.\")\n",
    "        print(\"También asegúrate de tener las bibliotecas 'transformers' y 'torch' instaladas.\")\n",
    "        raise\n",
    "\n",
    "def save_mel_filters(n_mels, save_dir):\n",
    "    \"\"\" Extrae los filtros Mel de la biblioteca Whisper y los guarda en formato txt. \"\"\"\n",
    "    print(f\"\\nExtrayendo y guardando {n_mels} filtros Mel...\")\n",
    "    if whisper_audio is None:\n",
    "        raise ImportError(\"El módulo 'whisper.audio' no está disponible. No se pueden extraer los filtros Mel.\")\n",
    "\n",
    "    filename = os.path.join(save_dir, f\"mel_{n_mels}_filters.txt\")\n",
    "    n_fft = N_FFT # Usar constante definida globalmente\n",
    "\n",
    "    try:\n",
    "        # Generar filtros usando la función de la biblioteca whisper\n",
    "        mel_filters_tensor = whisper_audio.mel_filters(device=DEVICE, n_mels=n_mels) # Usa n_fft=400 por defecto\n",
    "        # Aplanar el tensor y convertir a numpy array en CPU para guardarlo\n",
    "        mel_filters_flat = mel_filters_tensor.cpu().numpy().flatten()\n",
    "\n",
    "        # Guardar usando numpy.savetxt para control de formato preciso\n",
    "        # '%.18e' usa notación científica con 18 decimales\n",
    "        np.savetxt(filename, mel_filters_flat, fmt='%.18e', newline='\\n')\n",
    "        print(f\"Filtros Mel guardados en: {filename}\")\n",
    "        return filename\n",
    "    except AttributeError:\n",
    "        print(\"Error: No se pudo encontrar 'whisper.audio.mel_filters'. ¿Está instalada la biblioteca 'openai-whisper' correctamente?\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"Error extrayendo o guardando los filtros Mel: {e}\")\n",
    "        raise\n",
    "\n",
    "def save_vocabulary(processor, save_dir):\n",
    "    \"\"\" Extrae el vocabulario completo (base + añadido + timestamps) y lo guarda. \"\"\"\n",
    "    print(\"\\nExtrayendo y guardando el vocabulario completo...\")\n",
    "    if not hasattr(processor, 'tokenizer'):\n",
    "         raise ValueError(\"El objeto Processor no tiene un atributo 'tokenizer'.\")\n",
    "\n",
    "    tokenizer = processor.tokenizer\n",
    "    vocab_filename = os.path.join(save_dir, \"vocab_es.txt\") # Nombre específico\n",
    "\n",
    "    combined_vocab = {}\n",
    "    added_token_ids = set()\n",
    "\n",
    "    # Parámetros para generación de Timestamps (ajustados a Whisper)\n",
    "    TS_START_ID = 50364 # ID de <|0.00|>\n",
    "    TS_END_ID = 51864   # ID de <|30.00|>\n",
    "    NUM_TS_TOKENS = (TS_END_ID - TS_START_ID) + 1 # Debe ser 1501\n",
    "    TS_START_TIME_SEC = 0.00\n",
    "    TS_TIME_INCREMENT_SEC = 0.02 # Incremento de 20ms por token\n",
    "\n",
    "    print(f\"Parámetros de Timestamp: IDs {TS_START_ID}-{TS_END_ID} ({NUM_TS_TOKENS} tokens)\")\n",
    "\n",
    "    try:\n",
    "        # 1. Procesar tokens añadidos existentes (si los hay)\n",
    "        if hasattr(tokenizer, 'added_tokens_decoder') and tokenizer.added_tokens_decoder:\n",
    "            added_tokens_decoder = tokenizer.added_tokens_decoder\n",
    "            print(f\"Procesando {len(added_tokens_decoder)} tokens añadidos encontrados en el tokenizer...\")\n",
    "            for token_id, added_token_obj in added_tokens_decoder.items():\n",
    "                combined_vocab[token_id] = added_token_obj.content\n",
    "                added_token_ids.add(token_id)\n",
    "        else:\n",
    "            print(\"Advertencia: No se encontraron tokens añadidos explícitos ('added_tokens_decoder') en el tokenizer.\")\n",
    "\n",
    "        # 2. Comprobar y/o generar tokens de Timestamp\n",
    "        if TS_START_ID in added_token_ids:\n",
    "            print(\"-> Información: Los tokens de timestamp parecen estar presentes en los tokens añadidos.\")\n",
    "        else:\n",
    "            print(f\"-> Advertencia: El token de inicio de timestamp ID={TS_START_ID} ('<|0.00|>') NO se encontró.\")\n",
    "            print(f\"   Intentando AUTO-GENERAR {NUM_TS_TOKENS} tokens de timestamp.\")\n",
    "            generation_count = 0\n",
    "            for i in range(NUM_TS_TOKENS):\n",
    "                current_id = TS_START_ID + i\n",
    "                if current_id > TS_END_ID: break # Seguridad\n",
    "\n",
    "                current_time = TS_START_TIME_SEC + i * TS_TIME_INCREMENT_SEC\n",
    "                time_str = f\"{round(current_time, 2):.2f}\"\n",
    "                token_str = f\"<|{time_str}|>\"\n",
    "\n",
    "                if current_id in combined_vocab:\n",
    "                    print(f\"   Advertencia: Sobrescribiendo ID {current_id} ('{combined_vocab[current_id]}') con TS generado '{token_str}'\")\n",
    "                combined_vocab[current_id] = token_str\n",
    "                added_token_ids.add(current_id) # Marcar como añadido\n",
    "                generation_count += 1\n",
    "            print(f\"   Se generaron y añadieron {generation_count} tokens de timestamp.\")\n",
    "\n",
    "        # 3. Procesar el vocabulario base (excluyendo los añadidos/generados)\n",
    "        vocab_size = tokenizer.vocab_size\n",
    "        print(f\"Iterando hasta vocab_size ({vocab_size}) para encontrar tokens base (excluyendo {len(added_token_ids)} IDs)...\")\n",
    "        base_token_count = 0\n",
    "        for token_id in range(vocab_size):\n",
    "            if token_id not in added_token_ids:\n",
    "                token_str = tokenizer.convert_ids_to_tokens(token_id)\n",
    "                if token_str is not None:\n",
    "                    if token_id not in combined_vocab:\n",
    "                        combined_vocab[token_id] = token_str\n",
    "                        base_token_count += 1\n",
    "                    # else: # Muy raro que ocurra si la lógica es correcta\n",
    "                    #     print(f\"Advertencia: ID base {token_id} ('{token_str}') ya estaba en combined_vocab?\")\n",
    "                # else: # ID dentro de vocab_size pero sin representación?\n",
    "                #     print(f\"Advertencia: convert_ids_to_tokens devolvió None para ID base potencial {token_id}\")\n",
    "\n",
    "        print(f\"Se encontraron {base_token_count} tokens base.\")\n",
    "\n",
    "        # 4. Ordenar y escribir al archivo\n",
    "        if not combined_vocab:\n",
    "            raise ValueError(\"El vocabulario combinado está vacío después del procesamiento.\")\n",
    "\n",
    "        print(f\"Total de tokens únicos en el mapa combinado final: {len(combined_vocab)}\")\n",
    "        sorted_combined_items = sorted(combined_vocab.items(), key=lambda item: item[0]) # Ordenar por ID\n",
    "\n",
    "        with open(vocab_filename, 'w', encoding='utf-8') as f:\n",
    "            for token_id, token_str in sorted_combined_items:\n",
    "                f.write(f\"{token_id} {token_str}\\n\")\n",
    "\n",
    "        print(f\"Vocabulario completo guardado en: {vocab_filename}\")\n",
    "        return vocab_filename\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error extrayendo o guardando el vocabulario combinado: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "\n",
    "def setup_dummy_data(model, processor, n_mels):\n",
    "    \"\"\" Crea datos de entrada dummy con las dimensiones correctas para exportar ONNX. \"\"\"\n",
    "    print(\"\\nPreparando datos dummy para exportación ONNX...\")\n",
    "    if n_mels != processor.feature_extractor.feature_size:\n",
    "        warnings.warn(f\"n_mels especificado ({n_mels}) difiere del feature_size del procesador ({processor.feature_extractor.feature_size}). Usando el valor del procesador.\")\n",
    "        n_mels = processor.feature_extractor.feature_size\n",
    "\n",
    "    # Crear espectrograma Mel dummy (valores aleatorios)\n",
    "    # Dimensiones: [batch_size, n_mels, n_frames]\n",
    "    x_mel = torch.randn(FIXED_BATCH_SIZE, n_mels, FIXED_NUM_FRAMES, dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "    # Ejecutar el encoder una vez para obtener la salida (hidden_states) que necesita el decoder\n",
    "    print(f\"Ejecutando encoder con entrada dummy de forma: {x_mel.shape}\")\n",
    "    try:\n",
    "        with torch.no_grad(): # No necesitamos calcular gradientes\n",
    "             encoder_output_obj = model.model.encoder(x_mel)\n",
    "        encoder_hidden_states = encoder_output_obj.last_hidden_state # Tensor [batch, seq_len_encoder, d_model]\n",
    "        print(f\"Salida del Encoder (hidden_states) obtenida con forma: {encoder_hidden_states.shape}\")\n",
    "    except Exception as e:\n",
    "         print(f\"Error al ejecutar el encoder con datos dummy: {e}\")\n",
    "         raise\n",
    "\n",
    "    # Crear tokens de entrada dummy para el decoder\n",
    "    # Usamos una longitud corta representativa, p.ej., 12 tokens\n",
    "    # Dimensiones: [batch_size, sequence_length]\n",
    "    dummy_decoder_input_length = FIXED_DECODER_INPUT_LENGTH\n",
    "    x_tokens = torch.randint(0, model.config.vocab_size, (FIXED_BATCH_SIZE, dummy_decoder_input_length), dtype=torch.long).to(DEVICE)\n",
    "    print(f\"Tokens de entrada dummy para Decoder creados con forma: {x_tokens.shape}\")\n",
    "\n",
    "    print(\"\\n--- Formas de Datos Dummy ---\")\n",
    "    print(f\"  Entrada Encoder (x_mel): {x_mel.shape}, dtype: {x_mel.dtype}\")\n",
    "    print(f\"  Salida Encoder (encoder_hidden_states): {encoder_hidden_states.shape}, dtype: {encoder_hidden_states.dtype}\")\n",
    "    print(f\"  Entrada Decoder (x_tokens): {x_tokens.shape}, dtype: {x_tokens.dtype}\")\n",
    "    print(\"---------------------------\\n\")\n",
    "\n",
    "    return x_mel, encoder_hidden_states, x_tokens\n",
    "\n",
    "\n",
    "def add_shape_info(model_path_in, model_path_out):\n",
    "    \"\"\" Intenta añadir información de forma/tipo a un modelo ONNX. \"\"\"\n",
    "    try:\n",
    "        print(f\"Intentando añadir información de forma/tipo a: {os.path.basename(model_path_in)}\")\n",
    "        model = onnx.load(model_path_in)\n",
    "        # Eliminar información de forma existente si la hubiera (a veces causa problemas)\n",
    "        onnx.shape_inference.infer_shapes(model, check_type=True, strict_mode=False, data_prop=True)\n",
    "        # Guardar el modelo con la información inferida\n",
    "        onnx.save(model, model_path_out)\n",
    "        print(f\"-> Modelo con información de forma/tipo guardado en: {os.path.basename(model_path_out)}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"-> ¡Error! durante la inferencia/guardado de formas para {os.path.basename(model_path_in)}: {e}\")\n",
    "        # traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "def simplify_onnx_model(model_path):\n",
    "    \"\"\" Simplifica un modelo ONNX usando onnxsim y luego intenta añadir info de forma. \"\"\"\n",
    "    try:\n",
    "        print(f\"Simplificando modelo ONNX: {os.path.basename(model_path)}\")\n",
    "        original_model = onnx.load(model_path)\n",
    "        simplified_model, check = simplify(original_model) # onnxsim\n",
    "\n",
    "        if check:\n",
    "            print(\"-> Simplificación exitosa.\")\n",
    "            # Guardar el modelo simplificado temporalmente (sobrescribiendo el original)\n",
    "            onnx.save(simplified_model, model_path)\n",
    "            # Intentar añadir info de forma al modelo simplificado\n",
    "            if add_shape_info(model_path, model_path): # Sobrescribe de nuevo\n",
    "                 print(f\"-> Éxito: Modelo simplificado y con info de forma guardado: {os.path.basename(model_path)}\")\n",
    "            else:\n",
    "                 print(f\"-> Advertencia: Modelo simplificado pero falló al añadir info de forma: {os.path.basename(model_path)}\")\n",
    "                 # Nota: El archivo ahora contiene el modelo simplificado pero SIN la nueva info de forma.\n",
    "        else:\n",
    "            print(f\"-> ¡Error! No se pudo simplificar el modelo: {os.path.basename(model_path)}\")\n",
    "            # Intentar añadir info de forma al modelo original como fallback\n",
    "            print(\"-> Intentando añadir info de forma al modelo original...\")\n",
    "            if add_shape_info(model_path, model_path):\n",
    "                print(f\"-> Éxito (Fallback): Info de forma añadida al modelo original: {os.path.basename(model_path)}\")\n",
    "            else:\n",
    "                print(f\"-> Fallo Total: Ni simplificación ni info de forma para: {os.path.basename(model_path)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"-> ¡Error! durante la simplificación/info-forma de ONNX para {os.path.basename(model_path)}: {e}\")\n",
    "        # traceback.print_exc()\n",
    "\n",
    "\n",
    "# Wrapper para incluir la capa de proyección en la exportación del Decoder\n",
    "class DecoderWithProjectionWrapper(torch.nn.Module):\n",
    "    \"\"\" Envuelve el decoder y la capa de proyección final para exportarlos juntos. \"\"\"\n",
    "    def __init__(self, decoder, projection_layer):\n",
    "        super().__init__()\n",
    "        self.decoder = decoder\n",
    "        self.proj_out = projection_layer\n",
    "\n",
    "    def forward(self, input_ids, encoder_hidden_states):\n",
    "        # Pasar entradas al decoder base\n",
    "        decoder_outputs = self.decoder(\n",
    "            input_ids=input_ids,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            use_cache=False # Desactivar caché KV para exportación simple\n",
    "        )\n",
    "        # Obtener la última capa oculta del decoder\n",
    "        last_hidden_state = decoder_outputs[0]\n",
    "        # Aplicar la capa de proyección final para obtener los logits\n",
    "        logits = self.proj_out(last_hidden_state)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08285e78",
   "metadata": {},
   "source": [
    "Ejecución de paso 1 - Descarga de modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d5ba04d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "=== Iniciando Parte 1: Preparación del Modelo y Recursos ===\n",
      "============================================================\n",
      "Cargando modelo y procesador desde Hugging Face: rjac/whisper-tiny-spanish\n",
      "Modelo rjac/whisper-tiny-spanish, Procesador y Configuración cargados.\n",
      "\n",
      "--- Dimensiones Clave del Modelo ---\n",
      "  Nombre del Modelo: rjac/whisper-tiny-spanish\n",
      "  Batch Size (Fijo para Exportación): 1\n",
      "  Bandas Mel (N_MELS): 80\n",
      "  Frames Mel Fijos (Entrada Encoder): 3000\n",
      "  Longitud Secuencia Salida Encoder: 1500\n",
      "  Tamaño Oculto (d_model): 384\n",
      "  Tamaño Vocabulario: 51865\n",
      "  Máx Tokens Decoder: 448\n",
      "-----------------------------------\n",
      "\n",
      "\n",
      "Extrayendo y guardando 80 filtros Mel...\n",
      "Filtros Mel guardados en: ./model/mel_80_filters.txt\n",
      "\n",
      "Extrayendo y guardando el vocabulario completo...\n",
      "Parámetros de Timestamp: IDs 50364-51864 (1501 tokens)\n",
      "Procesando 107 tokens añadidos encontrados en el tokenizer...\n",
      "-> Advertencia: El token de inicio de timestamp ID=50364 ('<|0.00|>') NO se encontró.\n",
      "   Intentando AUTO-GENERAR 1501 tokens de timestamp.\n",
      "   Se generaron y añadieron 1501 tokens de timestamp.\n",
      "Iterando hasta vocab_size (50257) para encontrar tokens base (excluyendo 1608 IDs)...\n",
      "Se encontraron 50257 tokens base.\n",
      "Total de tokens únicos en el mapa combinado final: 51865\n",
      "Vocabulario completo guardado en: ./model/vocab_es.txt\n",
      "\n",
      "Preparando datos dummy para exportación ONNX...\n",
      "Ejecutando encoder con entrada dummy de forma: torch.Size([1, 80, 3000])\n",
      "Salida del Encoder (hidden_states) obtenida con forma: torch.Size([1, 1500, 384])\n",
      "Tokens de entrada dummy para Decoder creados con forma: torch.Size([1, 12])\n",
      "\n",
      "--- Formas de Datos Dummy ---\n",
      "  Entrada Encoder (x_mel): torch.Size([1, 80, 3000]), dtype: torch.float32\n",
      "  Salida Encoder (encoder_hidden_states): torch.Size([1, 1500, 384]), dtype: torch.float32\n",
      "  Entrada Decoder (x_tokens): torch.Size([1, 12]), dtype: torch.int64\n",
      "---------------------------\n",
      "\n",
      "\n",
      "Exportando Encoder a: whisper_encoder_rjac_whisper-tiny-spanish.onnx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manuel/.miniconda3/envs/rknn/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:1019: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if input_features.shape[-1] != expected_seq_length:\n",
      "/home/manuel/.miniconda3/envs/rknn/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:558: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_output.size() != (bsz, self.num_heads, tgt_len, self.head_dim):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Exportación del Encoder completada.\n",
      "Simplificando modelo ONNX: whisper_encoder_rjac_whisper-tiny-spanish.onnx\n",
      "-> Simplificación exitosa.\n",
      "Intentando añadir información de forma/tipo a: whisper_encoder_rjac_whisper-tiny-spanish.onnx\n",
      "-> Modelo con información de forma/tipo guardado en: whisper_encoder_rjac_whisper-tiny-spanish.onnx\n",
      "-> Éxito: Modelo simplificado y con info de forma guardado: whisper_encoder_rjac_whisper-tiny-spanish.onnx\n",
      "\n",
      "Exportando Decoder con Proyección a: whisper_decoder_rjac_whisper-tiny-spanish.onnx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manuel/.miniconda3/envs/rknn/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:1484: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if sequence_length != 1:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Exportación del Decoder con Proyección completada.\n",
      "Simplificando modelo ONNX: whisper_decoder_rjac_whisper-tiny-spanish.onnx\n",
      "-> Simplificación exitosa.\n",
      "Intentando añadir información de forma/tipo a: whisper_decoder_rjac_whisper-tiny-spanish.onnx\n",
      "-> Modelo con información de forma/tipo guardado en: whisper_decoder_rjac_whisper-tiny-spanish.onnx\n",
      "-> Éxito: Modelo simplificado y con info de forma guardado: whisper_decoder_rjac_whisper-tiny-spanish.onnx\n",
      "\n",
      "============================================================\n",
      "=== Resumen Parte 1 ===\n",
      "  Filtros Mel: Guardados en mel_80_filters.txt\n",
      "  Vocabulario: Guardado en vocab_es.txt\n",
      "  Encoder ONNX: Guardado en whisper_encoder_rjac_whisper-tiny-spanish.onnx\n",
      "  Decoder ONNX: Guardado en whisper_decoder_rjac_whisper-tiny-spanish.onnx\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"=== Iniciando Parte 1: Preparación del Modelo y Recursos ===\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "onnx_encoder_path = None\n",
    "onnx_decoder_path = None\n",
    "mel_filters_path = None\n",
    "vocab_path = None\n",
    "\n",
    "try:\n",
    "    # 1. Cargar Modelo y Procesador\n",
    "    model, processor, config = setup_model(DEFAULT_MODEL_NAME)\n",
    "    current_n_mels = processor.feature_extractor.feature_size # Confirmar N_MELS\n",
    "\n",
    "    # 2. Guardar Filtros Mel\n",
    "    mel_filters_path = save_mel_filters(current_n_mels, MODEL_SAVE_DIR)\n",
    "\n",
    "    # 3. Guardar Vocabulario\n",
    "    vocab_path = save_vocabulary(processor, MODEL_SAVE_DIR)\n",
    "\n",
    "    # 4. Preparar Datos Dummy para Exportación\n",
    "    x_mel_dummy, enc_hidden_dummy, x_tokens_dummy = setup_dummy_data(model, processor, current_n_mels)\n",
    "\n",
    "    # 5. Exportar Encoder\n",
    "    onnx_encoder_path = os.path.join(MODEL_SAVE_DIR, f\"whisper_encoder_{DEFAULT_MODEL_NAME.replace('/', '_')}.onnx\")\n",
    "    print(f\"\\nExportando Encoder a: {os.path.basename(onnx_encoder_path)}...\")\n",
    "    try:\n",
    "        torch.onnx.export(\n",
    "            model.model.encoder,        # El sub-módulo encoder\n",
    "            (x_mel_dummy),              # Argumentos de entrada (solo el mel espectrograma)\n",
    "            onnx_encoder_path,          # Ruta de guardado\n",
    "            input_names=[\"x\"],          # Nombre para la entrada Mel (importante para inferencia)\n",
    "            output_names=[\"out\"],       # Nombre para la salida (importante para inferencia)\n",
    "            opset_version=17,           # Versión del ONNX opset (17 es reciente y compatible)\n",
    "        )\n",
    "        print(\"-> Exportación del Encoder completada.\")\n",
    "        simplify_onnx_model(onnx_encoder_path) # Simplificar y añadir info de forma\n",
    "    except Exception as e:\n",
    "        print(f\"¡Error exportando o simplificando el Encoder!: {e}\")\n",
    "        onnx_encoder_path = None # Marcar como fallido\n",
    "\n",
    "    # 6. Exportar Decoder con Proyección\n",
    "    onnx_decoder_path = os.path.join(MODEL_SAVE_DIR, f\"whisper_decoder_{DEFAULT_MODEL_NAME.replace('/', '_')}.onnx\")\n",
    "    print(f\"\\nExportando Decoder con Proyección a: {os.path.basename(onnx_decoder_path)}...\")\n",
    "    try:\n",
    "        # Verificar si la capa de proyección existe\n",
    "        if not hasattr(model, 'proj_out'):\n",
    "                raise AttributeError(\"El modelo cargado no tiene el atributo 'proj_out' esperado para la capa de proyección final.\")\n",
    "\n",
    "        # Instanciar el wrapper que combina decoder y proyección\n",
    "        decoder_with_proj_for_export = DecoderWithProjectionWrapper(\n",
    "            model.model.decoder, # El sub-módulo decoder\n",
    "            model.proj_out       # La capa de proyección final\n",
    "        ).eval().to(DEVICE)\n",
    "\n",
    "        # Exportar el wrapper\n",
    "        torch.onnx.export(\n",
    "            decoder_with_proj_for_export,   # El módulo wrapper\n",
    "            (x_tokens_dummy, enc_hidden_dummy), # Argumentos: tokens de entrada, salida del encoder\n",
    "            onnx_decoder_path,              # Ruta de guardado\n",
    "            input_names=[\"tokens\", \"audio\"], # Nombres de entrada (importante)\n",
    "            output_names=[\"out\"],        # Nombre de salida (importante)\n",
    "            opset_version=17\n",
    "        )\n",
    "        print(\"-> Exportación del Decoder con Proyección completada.\")\n",
    "        simplify_onnx_model(onnx_decoder_path) # Simplificar y añadir info de forma\n",
    "    except Exception as e:\n",
    "        print(f\"¡Error exportando o simplificando el Decoder!: {e}\")\n",
    "        onnx_decoder_path = None # Marcar como fallido\n",
    "\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"=== Resumen Parte 1 ===\")\n",
    "    print(f\"  Filtros Mel: {'Guardados en ' + os.path.basename(mel_filters_path) if mel_filters_path else 'Fallo'}\")\n",
    "    print(f\"  Vocabulario: {'Guardado en ' + os.path.basename(vocab_path) if vocab_path else 'Fallo'}\")\n",
    "    print(f\"  Encoder ONNX: {'Guardado en ' + os.path.basename(onnx_encoder_path) if onnx_encoder_path else 'Fallo'}\")\n",
    "    print(f\"  Decoder ONNX: {'Guardado en ' + os.path.basename(onnx_decoder_path) if onnx_decoder_path else 'Fallo'}\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "    if not all([mel_filters_path, vocab_path, onnx_encoder_path, onnx_decoder_path]):\n",
    "            print(\"ADVERTENCIA: Uno o más artefactos no se pudieron generar correctamente.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"\\n¡¡¡ERROR CRÍTICO EN LA PARTE 1!!!\")\n",
    "    print(f\"Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    print(\"No se puede continuar a las siguientes partes si la preparación falla.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f446f5",
   "metadata": {},
   "source": [
    "## Paso 2 - Convertir archivos ONNX a RKNN\n",
    "\n",
    "Imports para el proceso de exportación a RKNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acf6e894",
   "metadata": {},
   "outputs": [],
   "source": [
    "import builtins, sys\n",
    "builtins.exit = sys.exit ## Requerido para evitar que RKNN llame a exit() y cierre el script\n",
    "\n",
    "import glob\n",
    "from rknn.api import RKNN\n",
    "\n",
    "# Valores por defecto para los parámetros\n",
    "DEFAULT_RKNN_DTYPE = 'fp'\n",
    "DEFAULT_RKNN_PLATFORM = 'rk3588'\n",
    "\n",
    "# Plataformas y tipos de datos soportados (ajusta según sea necesario)\n",
    "SUPPORTED_RKNN_PLATFORMS = ['rk3562', 'rk3566', 'rk3568', 'rk3576', 'rk3588']\n",
    "SUPPORTED_RKNN_DTYPES = ['i8', 'u8', 'fp']\n",
    "\n",
    "RKNN_DATASET_PATH = './dataset.txt' # Archivo requerido para RKNN si se implementa cuantización a INT8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f812420",
   "metadata": {},
   "source": [
    "Funciones utilitarias para el paso 2 - Convertir el modelo ONNX a RKNN\n",
    "\n",
    "1.  **Busca los archivos `.onnx`** generados en la Parte 1 (o colocados manualmente) en el directorio `MODEL_SAVE_DIR`.\n",
    "2.  **Configura la conversión** para una plataforma Rockchip específica (ej. `rk3588`) y un tipo de datos (`fp` para punto flotante, `i8`/`u8` para cuantización entera de 8 bits).\n",
    "3.  **Si se elige cuantización (`i8`/`u8`)**: Requiere un archivo `dataset.txt` que liste archivos de audio para la calibración. El script intentará usarlo.\n",
    "4.  **Itera sobre cada archivo `.onnx` encontrado**:\n",
    "     * Carga el modelo ONNX.\n",
    "     * Construye el modelo RKNN (aplicando cuantización si se especificó).\n",
    "     * Exporta el modelo resultante a formato `.rknn`.\n",
    "5.  **Muestra un resumen** de la conversión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b27e8eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_onnx_to_rknn(onnx_model_path, platform, dtype, save_dir, dataset_path=None):\n",
    "    \"\"\" Convierte un único modelo ONNX a formato RKNN. \"\"\"\n",
    "    if RKNN is None:\n",
    "        print(\"Error: La biblioteca RKNN no está disponible. No se puede realizar la conversión.\")\n",
    "        return None\n",
    "\n",
    "    model_name = os.path.basename(onnx_model_path)\n",
    "    output_path = os.path.join(save_dir, model_name.replace('.onnx', '.rknn'))\n",
    "    print(f\"\\n--- Procesando para RKNN: {model_name} ---\")\n",
    "    print(f\"  Plataforma: {platform}, DType: {dtype}\")\n",
    "\n",
    "    # Crear objeto RKNN\n",
    "    rknn = RKNN(verbose=False) # Poner verbose=True para más detalles\n",
    "\n",
    "    # Configuración Previa\n",
    "    print('--> Configurando modelo RKNN...')\n",
    "    # Especificar la plataforma destino es crucial\n",
    "    rknn.config(target_platform=platform)\n",
    "    print('    Hecho.')\n",
    "\n",
    "    # Cargar Modelo ONNX\n",
    "    print('--> Cargando modelo ONNX...')\n",
    "    ret = rknn.load_onnx(model=onnx_model_path)\n",
    "    if ret != 0:\n",
    "        print(f'    ¡ERROR al cargar {model_name}! Código: {ret}. ¿El archivo ONNX es válido y accesible?')\n",
    "        rknn.release()\n",
    "        return None\n",
    "    print('    Hecho.')\n",
    "\n",
    "    # Construir Modelo RKNN\n",
    "    print('--> Construyendo modelo RKNN...')\n",
    "    do_quant = dtype in ['i8', 'u8']\n",
    "    build_args = {'do_quantization': do_quant}\n",
    "\n",
    "    if do_quant:\n",
    "        print(f\"    Activando cuantización ({dtype}).\")\n",
    "        if dataset_path and os.path.exists(dataset_path):\n",
    "            print(f\"    Usando dataset de calibración: {dataset_path}\")\n",
    "            build_args['dataset'] = dataset_path\n",
    "            # ¡IMPORTANTE! El dataset.txt para Whisper debe contener RUTAS A *ARCHIVOS DE AUDIO* (.wav, .mp3, etc.)\n",
    "            # La herramienta RKNN internamente los procesará para generar los datos de calibración (espectrogramas Mel).\n",
    "            # No pongas rutas a archivos .txt de espectrogramas directamente.\n",
    "        else:\n",
    "            print(f\"    ¡ERROR! La cuantización ({dtype}) requiere un archivo 'dataset.txt' válido en '{dataset_path}'.\")\n",
    "            print(f\"    Crea este archivo con rutas a archivos de audio (.wav) para calibración.\")\n",
    "            rknn.release()\n",
    "            return None\n",
    "\n",
    "    ret = rknn.build(**build_args)\n",
    "    if ret != 0:\n",
    "        print(f'    ¡ERROR al construir {model_name}! Código: {ret}. ¿Problemas de memoria, ops no soportadas, o dataset incorrecto?')\n",
    "        rknn.release()\n",
    "        return None\n",
    "    print('    Hecho.')\n",
    "\n",
    "    # Exportar Modelo RKNN\n",
    "    print('--> Exportando modelo RKNN...')\n",
    "    ret = rknn.export_rknn(output_path)\n",
    "    if ret != 0:\n",
    "        print(f'    ¡ERROR al exportar a {output_path}! Código: {ret}. ¿Permisos de escritura?')\n",
    "        rknn.release()\n",
    "        return None\n",
    "    print(f'    Modelo exportado exitosamente a: {os.path.basename(output_path)}')\n",
    "    print('    Hecho.')\n",
    "\n",
    "    # Liberar recursos\n",
    "    rknn.release()\n",
    "    print(f\"--- Fin Procesamiento RKNN (ÉXITO): {model_name} ---\\n\")\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a403c35",
   "metadata": {},
   "source": [
    "Ejecución de conversión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2de831ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I rknn-toolkit2 version: 2.3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "=== Iniciando Parte 2: Conversión de ONNX a RKNN ===\n",
      "============================================================\n",
      "Buscando archivos .onnx en: ./model\n",
      "\n",
      "Se encontraron 2 modelos ONNX para convertir:\n",
      "  - whisper_encoder_rjac_whisper-tiny-spanish.onnx\n",
      "  - whisper_decoder_rjac_whisper-tiny-spanish.onnx\n",
      "\n",
      "Configuración de Conversión:\n",
      "  Plataforma Destino: rk3588\n",
      "  Tipo de Datos (dtype): fp\n",
      "  Cuantización Desactivada (FP16/BF16 en NPU o FP32 en CPU).\n",
      "\n",
      "Iniciando proceso de conversión...\n",
      "\n",
      "\n",
      "--- Procesando para RKNN: whisper_encoder_rjac_whisper-tiny-spanish.onnx ---\n",
      "  Plataforma: rk3588, DType: fp\n",
      "--> Configurando modelo RKNN...\n",
      "    Hecho.\n",
      "--> Cargando modelo ONNX...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I Loading : 100%|████████████████████████████████████████████████| 67/67 [00:00<00:00, 11808.49it/s]\n",
      "\u001b[1;33mW\u001b[0m \u001b[1;33mload_onnx: The config.mean_values is None, zeros will be set for input 0!\u001b[0m\n",
      "\u001b[1;33mW\u001b[0m \u001b[1;33mload_onnx: The config.std_values is None, ones will be set for input 0!\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Hecho.\n",
      "--> Construyendo modelo RKNN...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I OpFusing 0 :  27%|████████████▋                                  | 27/100 [00:00<00:01, 51.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I OpFusing 2 : 100%|█████████████████████████████████████████████| 100/100 [00:00<00:00, 133.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I OpFusing 2 : 100%|██████████████████████████████████████████████| 100/100 [00:01<00:00, 60.51it/s]\n",
      "I rknn building ...\n",
      "I rknn building done.\n",
      "I rknn-toolkit2 version: 2.3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Hecho.\n",
      "--> Exportando modelo RKNN...\n",
      "    Modelo exportado exitosamente a: whisper_encoder_rjac_whisper-tiny-spanish.rknn\n",
      "    Hecho.\n",
      "--- Fin Procesamiento RKNN (ÉXITO): whisper_encoder_rjac_whisper-tiny-spanish.onnx ---\n",
      "\n",
      "\n",
      "--- Procesando para RKNN: whisper_decoder_rjac_whisper-tiny-spanish.onnx ---\n",
      "  Plataforma: rk3588, DType: fp\n",
      "--> Configurando modelo RKNN...\n",
      "    Hecho.\n",
      "--> Cargando modelo ONNX...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I Loading : 100%|████████████████████████████████████████████████| 102/102 [00:00<00:00, 993.08it/s]\n",
      "\u001b[1;33mW\u001b[0m \u001b[1;33mload_onnx: The config.mean_values is None, zeros will be set for input 1!\u001b[0m\n",
      "\u001b[1;33mW\u001b[0m \u001b[1;33mload_onnx: The config.std_values is None, ones will be set for input 1!\u001b[0m\n",
      "\u001b[1;33mW\u001b[0m \u001b[1;33mbuild: For tensor ['/decoder/layers.0/self_attn/Slice_output_0'], the value smaller than -3e+38 has been corrected to -10000. Set opt_level to 2 or lower to disable this correction.\u001b[0m\n",
      "\u001b[1;33mW\u001b[0m \u001b[1;33mbuild: For tensor ['/decoder/layers.0/self_attn/Slice_output_0_1'], the value smaller than -3e+38 has been corrected to -10000. Set opt_level to 2 or lower to disable this correction.\u001b[0m\n",
      "\u001b[1;33mW\u001b[0m \u001b[1;33mbuild: For tensor ['/decoder/layers.0/self_attn/Slice_output_0_2'], the value smaller than -3e+38 has been corrected to -10000. Set opt_level to 2 or lower to disable this correction.\u001b[0m\n",
      "\u001b[1;33mW\u001b[0m \u001b[1;33mbuild: For tensor ['/decoder/layers.0/self_attn/Slice_output_0_3'], the value smaller than -3e+38 has been corrected to -10000. Set opt_level to 2 or lower to disable this correction.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Hecho.\n",
      "--> Construyendo modelo RKNN...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I OpFusing 1 :  98%|█████████████████████████████████████████████ | 98/100 [00:00<00:00, 138.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I OpFusing 0 :   1%|▍                                               | 1/100 [00:00<01:36,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I OpFusing 2 : 100%|██████████████████████████████████████████████| 100/100 [00:01<00:00, 71.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I OpFusing 2 : 100%|██████████████████████████████████████████████| 100/100 [00:01<00:00, 61.44it/s]\n",
      "I rknn building ...\n",
      "E RKNN: [13:29:05.012] channel is too large, may produce thousands of regtask, fallback to cpu!\n",
      "E RKNN: [13:29:05.012] channel is too large, may produce thousands of regtask, fallback to cpu!\n",
      "E RKNN: [13:29:05.012] channel is too large, may produce thousands of regtask, fallback to cpu!\n",
      "E RKNN: [13:29:05.012] channel is too large, may produce thousands of regtask, fallback to cpu!\n",
      "E RKNN: [13:29:05.044] channel is too large, may produce thousands of regtask, fallback to cpu!\n",
      "I rknn building done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Hecho.\n",
      "--> Exportando modelo RKNN...\n",
      "    Modelo exportado exitosamente a: whisper_decoder_rjac_whisper-tiny-spanish.rknn\n",
      "    Hecho.\n",
      "--- Fin Procesamiento RKNN (ÉXITO): whisper_decoder_rjac_whisper-tiny-spanish.onnx ---\n",
      "\n",
      "\n",
      "============================================================\n",
      "=== Resumen Parte 2: Conversión RKNN ===\n",
      "  Modelos ONNX encontrados: 2\n",
      "  Conversiones Exitosas: 2\n",
      "    - whisper_encoder_rjac_whisper-tiny-spanish.rknn\n",
      "    - whisper_decoder_rjac_whisper-tiny-spanish.rknn\n",
      "  Conversiones Fallidas: 0\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"=== Iniciando Parte 2: Conversión de ONNX a RKNN ===\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if RKNN is None:\n",
    "    print(\"ADVERTENCIA: La biblioteca RKNN no está disponible. Saltando Parte 2.\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Validar plataforma y dtype\n",
    "if DEFAULT_RKNN_PLATFORM not in SUPPORTED_RKNN_PLATFORMS:\n",
    "    print(f\"ERROR: Plataforma RKNN inválida '{DEFAULT_RKNN_PLATFORM}'. Soportadas: {SUPPORTED_RKNN_PLATFORMS}\")\n",
    "\n",
    "if DEFAULT_RKNN_DTYPE not in SUPPORTED_RKNN_DTYPES:\n",
    "    print(f\"ERROR: dtype RKNN inválido '{DEFAULT_RKNN_DTYPE}'. Soportados: {SUPPORTED_RKNN_DTYPES}\")\n",
    "\n",
    "\n",
    "print(f\"Buscando archivos .onnx en: {MODEL_SAVE_DIR}\")\n",
    "onnx_files = glob.glob(os.path.join(MODEL_SAVE_DIR, '*.onnx'))\n",
    "\n",
    "if not onnx_files:\n",
    "    print(f\"No se encontraron archivos .onnx en '{MODEL_SAVE_DIR}'. Asegúrate de que la Parte 1 se completó correctamente.\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "\n",
    "print(f\"\\nSe encontraron {len(onnx_files)} modelos ONNX para convertir:\")\n",
    "for model_path in onnx_files:\n",
    "    print(f\"  - {os.path.basename(model_path)}\")\n",
    "\n",
    "print(f\"\\nConfiguración de Conversión:\")\n",
    "print(f\"  Plataforma Destino: {DEFAULT_RKNN_PLATFORM}\")\n",
    "print(f\"  Tipo de Datos (dtype): {DEFAULT_RKNN_DTYPE}\")\n",
    "if DEFAULT_RKNN_DTYPE in ['i8', 'u8']:\n",
    "        print(f\"  Cuantización Activada. Dataset esperado en: {RKNN_DATASET_PATH}\")\n",
    "        if not os.path.exists(RKNN_DATASET_PATH):\n",
    "            print(f\"  ADVERTENCIA: ¡El archivo dataset '{RKNN_DATASET_PATH}' no existe!\")\n",
    "            print(f\"               La conversión fallará si el dataset es requerido.\")\n",
    "else:\n",
    "        print(f\"  Cuantización Desactivada (FP16/BF16 en NPU o FP32 en CPU).\")\n",
    "\n",
    "print(\"\\nIniciando proceso de conversión...\\n\")\n",
    "\n",
    "rknn_success_files = []\n",
    "rknn_fail_files = []\n",
    "\n",
    "for onnx_path in onnx_files:\n",
    "    rknn_output_path = convert_onnx_to_rknn(onnx_path, DEFAULT_RKNN_PLATFORM, DEFAULT_RKNN_DTYPE, MODEL_SAVE_DIR, RKNN_DATASET_PATH)\n",
    "    if rknn_output_path:\n",
    "        rknn_success_files.append(rknn_output_path)\n",
    "    else:\n",
    "        rknn_fail_files.append(os.path.basename(onnx_path)) # Guardar nombre del onnx que falló\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"=== Resumen Parte 2: Conversión RKNN ===\")\n",
    "print(f\"  Modelos ONNX encontrados: {len(onnx_files)}\")\n",
    "print(f\"  Conversiones Exitosas: {len(rknn_success_files)}\")\n",
    "for fpath in rknn_success_files: print(f\"    - {os.path.basename(fpath)}\")\n",
    "print(f\"  Conversiones Fallidas: {len(rknn_fail_files)}\")\n",
    "for fname in rknn_fail_files: print(f\"    - {fname} (ONNX)\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "if rknn_fail_files:\n",
    "        print(\"ADVERTENCIA: Algunos modelos no pudieron ser convertidos a RKNN.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311cf05d",
   "metadata": {},
   "source": [
    "Si en este paso la conversión ha sido exitosa, es probable que el modelo RKNN esté listo para ser ejecutado en el chip. Sin embargo, en mi experiencia tuve muchos falsos positivos, en general los problemas se derivan de los shapes de los modelos ONNX, hice experimentos con Optimum y la preservación de los shapes de entrada y salida de los grafos y sus nodos son un dolor de cabeza. Si experimentas problemas sugiero revisar con Netron los grafos, puedes usar los ONNX proveidos como ejemplo por parte de Rockchip en rknn_model_zoo demo whisper para comparar, aunque este modelo está recortado de 20 segundos de audio, por tanto las dimensiones de muchos tensores van a diferir pero da una buena idea de lo que se espera por parte de RKNN Toolkit para hacer una conversión adecuada.\n",
    "\n",
    "## Paso 3. Evaluar ONNX en local\n",
    "\n",
    "Imports necesarios para esta fase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fcacc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "import scipy # Necesario para resample\n",
    "import soundfile as sf\n",
    "\n",
    "SAMPLE_RATE = 16000         # Tasa de muestreo esperada por Whisper (Hz)\n",
    "N_FFT = 400                 # Tamaño de la ventana para la Transformada Rápida de Fourier (STFT)\n",
    "HOP_LENGTH = 160            # Desplazamiento entre ventanas STFT consecutivas\n",
    "CHUNK_LENGTH = 30           # Duración estándar de un segmento de audio procesado por Whisper (segundos)\n",
    "N_SAMPLES = CHUNK_LENGTH * SAMPLE_RATE # Número total de muestras de audio en un chunk de 30s\n",
    "# MAX_LENGTH: Longitud máxima de la secuencia de frames de Mel que espera el encoder.\n",
    "# Se calcula como: (CHUNK_LENGTH * SAMPLE_RATE) / HOP_LENGTH -> 3000 frames.\n",
    "MAX_LENGTH = 3000\n",
    "N_MELS = 80   \n",
    "TASK_CODE_SPANISH=50262\n",
    "\n",
    "# Archivo de audio de ejemplo para inferencia (¡Cámbialo por tu archivo!)\n",
    "DEFAULT_AUDIO_PATH_FOR_INFERENCE = \"./test_es.wav\" \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cdfff1",
   "metadata": {},
   "source": [
    "Funciones utilitarias\n",
    "\n",
    "1.  **Define Constantes y Funciones de Preprocesamiento de Audio:**\n",
    "     * Constantes como `SAMPLE_RATE`, `N_FFT`, `HOP_LENGTH`, `MAX_MEL_LENGTH`, `N_MELS`.\n",
    "     * Funciones para asegurar que el audio esté en mono (`ensure_channels`) y a 16000 Hz (`ensure_sample_rate`).\n",
    "2.  **Define Funciones de Procesamiento de Espectrograma:**\n",
    "     * Función para cargar los filtros Mel (`load_mel_filters`) desde el archivo `.txt` generado en la Parte 1.\n",
    "     * Función para calcular el espectrograma Log-Mel (`log_mel_spectrogram`) usando PyTorch y los filtros cargados.\n",
    "     * Función para ajustar (padding o recorte) el espectrograma a la longitud fija `MAX_MEL_LENGTH` (`pad_or_trim`).\n",
    "3.  **Define Funciones de Vocabulario e Inferencia:**\n",
    "     * Función para cargar el vocabulario (`read_vocab`) desde el archivo `vocab_es.txt` generado en la Parte 1.\n",
    "     * Funciones para inicializar (`init_model`) y liberar (`release_model`) los modelos, soportando tanto ONNX (`onnxruntime`) como RKNN (`rknn-toolkit2`).\n",
    "     * Función para ejecutar el encoder (`run_encoder`) con el espectrograma Mel preprocesado.\n",
    "     * Función para ejecutar el decoder (`run_decoder`) de forma auto-regresiva, generando tokens hasta predecir el token de fin de texto (`<|endoftext|>`) o alcanzar un límite. Utiliza el vocabulario para decodificar los IDs de token a texto.\n",
    "4.  **Ejecución Principal de Inferencia:**\n",
    "     * Carga el archivo de audio especificado.\n",
    "     * Preprocesa el audio (mono, 16kHz).\n",
    "     * Calcula y ajusta el espectrograma Log-Mel.\n",
    "     * Carga el vocabulario.\n",
    "     * Inicializa los modelos (RKNN si están disponibles y se prefiere, si no ONNX).\n",
    "     * Ejecuta el encoder.\n",
    "     * Ejecuta el decoder para obtener la transcripción.\n",
    "     * Imprime la transcripción resultante.\n",
    "     * Libera los recursos del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f6465a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_sample_rate(waveform, original_sample_rate, desired_sample_rate=SAMPLE_RATE):\n",
    "    \"\"\"\n",
    "    Remuestrea la forma de onda de audio a la tasa de muestreo deseada (SAMPLE_RATE).\n",
    "    Utiliza scipy.signal.resample para alta calidad.\n",
    "    \"\"\"\n",
    "    if original_sample_rate != desired_sample_rate:\n",
    "        print(f\"Remuestreando audio: {original_sample_rate} Hz -> {desired_sample_rate} Hz\")\n",
    "        desired_length = int(round(float(len(waveform)) / original_sample_rate * desired_sample_rate))\n",
    "        waveform = scipy.signal.resample(waveform, desired_length)\n",
    "        print(\"Remuestreo completado.\")\n",
    "    # Convierte a float32 por si acaso resample cambia el tipo\n",
    "    return waveform.astype(np.float32), desired_sample_rate\n",
    "\n",
    "def ensure_channels(waveform, original_channels, desired_channels=1):\n",
    "    \"\"\"\n",
    "    Convierte el audio al número deseado de canales (generalmente a mono).\n",
    "    Promedia los canales si hay más de los deseados.\n",
    "    \"\"\"\n",
    "    # Primero, inferir el número de canales si no se dio explícitamente\n",
    "    inferred_channels = 1\n",
    "    if waveform.ndim > 1:\n",
    "        # Asumir que el eje más corto es el de canales (más robusto)\n",
    "        if waveform.shape[0] < waveform.shape[1]: # Formato (canales, muestras)\n",
    "            inferred_channels = waveform.shape[0]\n",
    "        else: # Formato (muestras, canales)\n",
    "            inferred_channels = waveform.shape[1]\n",
    "    elif waveform.ndim == 1:\n",
    "        inferred_channels = 1\n",
    "    else: # ndim == 0 (escalar) o > 2, raro para audio\n",
    "        print(f\"Advertencia: Forma de onda con dimensiones inesperadas: {waveform.shape}. Asumiendo 1 canal.\")\n",
    "        inferred_channels = 1\n",
    "\n",
    "    if inferred_channels > desired_channels:\n",
    "        print(f\"Convirtiendo canales: {inferred_channels} -> {desired_channels} (Mono)\")\n",
    "        # Intenta promediar a lo largo del eje correcto\n",
    "        if waveform.ndim > 1:\n",
    "             if waveform.shape[0] == inferred_channels: # Formato (canales, muestras)\n",
    "                  waveform = np.mean(waveform, axis=0)\n",
    "             elif waveform.shape[1] == inferred_channels: # Formato (muestras, canales)\n",
    "                  waveform = np.mean(waveform, axis=-1)\n",
    "             else: # No coincide con ninguna suposición\n",
    "                 print(\"Advertencia: No se pudo determinar el eje de canales para promediar. Se devolverá como está.\")\n",
    "                 return waveform, inferred_channels\n",
    "        # Si waveform.ndim era 1 pero inferred_channels>1, algo iba mal antes.\n",
    "        # Ya estamos en mono si ndim es 1.\n",
    "        print(\"Conversión a mono completada.\")\n",
    "        return waveform, desired_channels\n",
    "\n",
    "    elif inferred_channels < desired_channels:\n",
    "         # No se puede crear canales de la nada de forma significativa\n",
    "         print(f\"Advertencia: El audio tiene {inferred_channels} canales, se requieren {desired_channels}. No se puede convertir.\")\n",
    "         return waveform, inferred_channels\n",
    "    else:\n",
    "        # El número de canales ya es el deseado\n",
    "        return waveform, inferred_channels\n",
    "\n",
    "# --- Funciones Relacionadas con Vocabulario ---\n",
    "\n",
    "def read_vocab(vocab_path):\n",
    "    \"\"\"\n",
    "    Lee el archivo de vocabulario (formato: ID<espacio>TOKEN por línea).\n",
    "    Mapea los IDs (como string) a los tokens (string).\n",
    "    \"\"\"\n",
    "    if not os.path.exists(vocab_path):\n",
    "         raise FileNotFoundError(f\"Archivo de vocabulario no encontrado en: {vocab_path}\")\n",
    "    vocab = {}\n",
    "    print(f\"Leyendo vocabulario desde: {vocab_path}...\")\n",
    "    with open(vocab_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(' ', 1) # Divide en el primer espacio: ID TOKEN\n",
    "            if len(parts) == 2:\n",
    "                token_id_str = parts[0]\n",
    "                token_str = parts[1]\n",
    "                vocab[token_id_str] = token_str\n",
    "            elif len(parts) == 1 and parts[0]: # Asegura que no sea una línea vacía\n",
    "                 # Maneja casos donde podría haber solo un ID (ej. token vacío o especial sin espacio)\n",
    "                 token_id_str = parts[0]\n",
    "                 vocab[token_id_str] = \"\" # Asigna un string vacío\n",
    "            # Ignora líneas vacías o mal formateadas silenciosamente\n",
    "    print(f\"Vocabulario leído. {len(vocab)} tokens cargados.\")\n",
    "    return vocab\n",
    "\n",
    "# --- Funciones de Procesamiento de Espectrograma Mel ---\n",
    "\n",
    "def pad_or_trim(mel_array, length=MAX_LENGTH):\n",
    "    \"\"\"\n",
    "    Ajusta (rellena o recorta) el espectrograma Mel a una longitud fija (MAX_LENGTH).\n",
    "    \"\"\"\n",
    "    original_length = mel_array.shape[1] # Longitud temporal original\n",
    "\n",
    "    if original_length > length:\n",
    "        # Recortar: Selecciona solo los primeros 'length' frames temporales\n",
    "        print(f\"Recortando espectrograma Mel de {original_length} a {length} frames.\")\n",
    "        return mel_array[:, :length]\n",
    "    elif original_length < length:\n",
    "        # Rellenar (Padding): Añade columnas de valores al final\n",
    "        pad_width = length - original_length\n",
    "        print(f\"Rellenando espectrograma Mel de {original_length} a {length} frames.\")\n",
    "        # Usamos -1.0 como valor de padding, relacionado con la normalización de Whisper.\n",
    "        return np.pad(mel_array, ((0, 0), (0, pad_width)), mode='constant', constant_values=-1.0)\n",
    "    else:\n",
    "        # La longitud ya es la correcta\n",
    "        return mel_array\n",
    "\n",
    "def load_mel_filters(n_mels=N_MELS, filters_path=\"./model/mel_80_filters.txt\"):\n",
    "    \"\"\"\n",
    "    Carga los pesos del banco de filtros Mel precalculados desde un archivo de texto.\n",
    "    Espera un array plano que se remodela a [n_mels, n_fft // 2 + 1].\n",
    "    \"\"\"\n",
    "    if not os.path.exists(filters_path):\n",
    "        raise FileNotFoundError(f\"Archivo de filtros Mel no encontrado en: {filters_path}\")\n",
    "    try:\n",
    "        expected_columns = N_FFT // 2 + 1 # 201 para N_FFT=400\n",
    "        mels_data = np.loadtxt(filters_path, dtype=np.float32)\n",
    "        expected_size = n_mels * expected_columns\n",
    "        if mels_data.size != expected_size:\n",
    "             raise ValueError(f\"Tamaño inesperado de datos en {filters_path}. \"\n",
    "                              f\"Se esperaban {expected_size} elementos, se encontraron {mels_data.size}\")\n",
    "        mels_data = mels_data.reshape((n_mels, expected_columns))\n",
    "        print(f\"Filtros Mel ({mels_data.shape}) cargados desde {filters_path}\")\n",
    "        return torch.from_numpy(mels_data) # Devuelve Tensor PyTorch\n",
    "    except Exception as e:\n",
    "        print(f\"Error cargando o remodelando filtros Mel desde {filters_path}: {e}\")\n",
    "        raise\n",
    "\n",
    "def log_mel_spectrogram(audio, n_mels=N_MELS, filters=None, filters_path=\"./model/mel_80_filters.txt\"):\n",
    "    \"\"\"\n",
    "    Calcula el espectrograma Log-Mel usando PyTorch, replicando Whisper.\n",
    "    \"\"\"\n",
    "    # Asegura que el audio sea un tensor de PyTorch\n",
    "    if not torch.is_tensor(audio):\n",
    "        audio = torch.from_numpy(audio)\n",
    "\n",
    "    # Carga los filtros Mel si no se proporcionaron\n",
    "    if filters is None:\n",
    "        print(\"Cargando filtros Mel predeterminados...\")\n",
    "        filters = load_mel_filters(n_mels, filters_path) # Usa la función anterior para cargar\n",
    "\n",
    "    filters = filters.to(audio.device) # Mismo dispositivo que el audio\n",
    "    window = torch.hann_window(N_FFT).to(audio.device) # Ventana Hann\n",
    "\n",
    "    # STFT\n",
    "    stft_result = torch.stft(audio, n_fft=N_FFT, hop_length=HOP_LENGTH,\n",
    "                             window=window, return_complex=True, center=True)\n",
    "\n",
    "    # Espectrograma de Potencia (Magnitudes al cuadrado)\n",
    "    # Whisper usa stft[..., :-1], que son N_FFT // 2 bins (400 // 2 = 200 bins)\n",
    "    # Sin embargo, el filtro precalculado suele ser [80, 201]. Verifiquemos.\n",
    "    # Si filters es [80, 201], necesitamos magnitudes [201, T]\n",
    "    # stft_result es [201, T]. Entonces usamos stft_result.abs()**2\n",
    "    # PERO: El código original usa stft[..., :-1].abs() ** 2 -> [200, T]\n",
    "    # Y luego filters @ magnitudes. Esto implica que filters debe ser [80, 200].\n",
    "    # Vamos a seguir el código original: stft[..., :-1] y asumir filtros [80, 200].\n",
    "    # Si `load_mel_filters` carga [80, 201] y da error de tamaño, hay que ajustar\n",
    "    # o `load_mel_filters` o esta línea. El error original no era aquí, así que\n",
    "    # mantenemos la línea original por ahora.\n",
    "    magnitudes = stft_result[..., :-1].abs() ** 2 # Shape: [..., 200, T]\n",
    "\n",
    "    # Verifica compatibilidad de dimensiones (Defensivo)\n",
    "    if filters.shape[1] != magnitudes.shape[-2]:\n",
    "         print(f\"Advertencia: Incompatibilidad de dimensiones entre Filtros Mel {filters.shape} y Magnitudes STFT {magnitudes.shape}. \"\n",
    "               f\"Se esperaba Filtros[1] == Magnitudes[-2]. Intentando continuar...\")\n",
    "         # Podría fallar en la línea siguiente si las dimensiones son incompatibles.\n",
    "\n",
    "    # Aplicar filtros Mel\n",
    "    mel_spec = filters @ magnitudes\n",
    "\n",
    "    # Logaritmo y Clamping\n",
    "    log_spec = torch.clamp(mel_spec, min=1e-10).log10()\n",
    "\n",
    "    # Normalización Whisper\n",
    "    log_spec = torch.maximum(log_spec, log_spec.max() - 8.0)\n",
    "    log_spec = (log_spec + 4.0) / 4.0\n",
    "\n",
    "    print(f\"Espectrograma Log-Mel calculado. Forma: {log_spec.shape}\")\n",
    "    return log_spec\n",
    "\n",
    "# --- Funciones de Inferencia del Modelo ---\n",
    "\n",
    "def run_encoder(encoder_model, mel_input):\n",
    "    \"\"\"\n",
    "    Ejecuta la inferencia del modelo Encoder (ONNX o RKNN).\n",
    "    \"\"\"\n",
    "    expected_shape = (1, N_MELS, MAX_LENGTH)\n",
    "    if mel_input.shape != expected_shape:\n",
    "         raise ValueError(f\"Input de Encoder con forma inesperada: {mel_input.shape}. Se esperaba {expected_shape}\")\n",
    "\n",
    "    if isinstance(encoder_model, RKNN):\n",
    "        print(\"Ejecutando inferencia RKNN (Encoder)...\")\n",
    "        outputs = encoder_model.inference(inputs=[mel_input.astype(np.float32)]) # Asegura float32\n",
    "        out_encoder = outputs[0]\n",
    "    elif isinstance(encoder_model, onnxruntime.InferenceSession):\n",
    "        print(\"Ejecutando inferencia ONNX (Encoder)...\")\n",
    "        input_name = encoder_model.get_inputs()[0].name\n",
    "        output_name = encoder_model.get_outputs()[0].name\n",
    "        print(f\"  Input ONNX: '{input_name}', Output ONNX: '{output_name}'\")\n",
    "        outputs = encoder_model.run([output_name], {input_name: mel_input.astype(np.float32)}) # Asegura float32\n",
    "        out_encoder = outputs[0]\n",
    "    else:\n",
    "        raise TypeError(\"Tipo de modelo Encoder no soportado. Debe ser RKNN o ONNX InferenceSession.\")\n",
    "\n",
    "    print(f\"Salida del Encoder obtenida. Forma: {out_encoder.shape}\")\n",
    "    return out_encoder\n",
    "\n",
    "# def _decode_step(...):  # Eliminada porque no se usaba y la lógica estaba en _decode\n",
    "\n",
    "# --- REVERTIDA A LA LÓGICA ORIGINAL ---\n",
    "def _decode(decoder_model, tokens, out_encoder):\n",
    "    \"\"\"\n",
    "    Realiza UN PASO de inferencia del modelo Decoder (ONNX o RKNN).\n",
    "    Esta es la versión que coincide con la lógica original del script,\n",
    "    usando chequeo de tipo por string y nombres de input hardcodeados.\n",
    "\n",
    "    Args:\n",
    "        decoder_model (RKNN or onnxruntime.InferenceSession): Modelo decoder cargado.\n",
    "        tokens (list): Lista de IDs de token actuales (int).\n",
    "        out_encoder (np.ndarray): Salida del modelo encoder.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Logits predichos por el decoder, con forma\n",
    "                    (1, seq_len, vocab_size).\n",
    "    \"\"\"\n",
    "    # Prepara el input de tokens como un array numpy (1, seq_len) de int64\n",
    "    tokens_array = np.asarray([tokens], dtype=np.int64)\n",
    "\n",
    "    # Ejecuta inferencia según el tipo de modelo (usando chequeo por string como en el original)\n",
    "    if 'rknn' in str(type(decoder_model)).lower(): # Chequeo insensible a mayúsculas\n",
    "        # RKNN espera una lista de inputs: [tokens_array, encoder_output]\n",
    "        # print(f\"  Input RKNN Decoder: tokens={tokens_array.shape}, encoder_out={out_encoder.shape}\")\n",
    "        outputs = decoder_model.inference(inputs=[tokens_array, out_encoder])\n",
    "        out_decoder = outputs[0]\n",
    "    elif 'onnxruntime' in str(type(decoder_model)).lower(): # Chequeo insensible a mayúsculas\n",
    "        # ONNX espera un diccionario {nombre_input: valor_input}\n",
    "        # Usa los nombres hardcodeados del script original (\"tokens\", \"audio\")\n",
    "        # Asegúrate que estos nombres coinciden con los de TU modelo ONNX decoder.\n",
    "        input_dict = {\n",
    "            \"tokens\": tokens_array,\n",
    "            \"audio\": out_encoder\n",
    "        }\n",
    "        # print(f\"  Input ONNX Decoder: tokens={tokens_array.shape}, audio={out_encoder.shape}\")\n",
    "        # `run(None, ...)` ejecuta todas las salidas\n",
    "        outputs = decoder_model.run(None, input_dict)\n",
    "        out_decoder = outputs[0] # Asume que los logits son la primera salida\n",
    "    else:\n",
    "        # Si no es ni RKNN ni ONNX, lanza un error.\n",
    "        # Usamos isinstance para chequeo futuro, aunque el if/elif anterior lo cubre.\n",
    "        if not isinstance(decoder_model, (RKNN, onnxruntime.InferenceSession)):\n",
    "             raise TypeError(\"Tipo de modelo Decoder no soportado. Debe ser RKNN o ONNX InferenceSession.\")\n",
    "        # Si llegara aquí por alguna razón inesperada\n",
    "        print(f\"Advertencia: Tipo de modelo decoder detectado como {type(decoder_model)}, no manejado explícitamente por el chequeo de string. Intentando continuar...\")\n",
    "        # Intenta como ONNX por defecto (o lanza excepción si falla)\n",
    "        try:\n",
    "            input_dict = {\"tokens\": tokens_array, \"audio\": out_encoder}\n",
    "            outputs = decoder_model.run(None, input_dict)\n",
    "            out_decoder = outputs[0]\n",
    "        except Exception as e:\n",
    "             raise TypeError(f\"No se pudo ejecutar inferencia con el tipo de modelo {type(decoder_model)}: {e}\")\n",
    "\n",
    "\n",
    "    # print(f\"  Logits obtenidos. Forma: {out_decoder.shape}\")\n",
    "    return out_decoder\n",
    "\n",
    "\n",
    "def run_decoder(decoder_model, out_encoder, vocab, task_code):\n",
    "    \"\"\"\n",
    "    Realiza el proceso de decodificación autoregresiva (greedy search).\n",
    "    Genera texto token a token hasta encontrar <|eot|> o alcanzar límite.\n",
    "    Usa la lógica específica de manejo de tokens del script original.\n",
    "    \"\"\"\n",
    "    # Asegura que task_code sea un entero (Corrección del error de tupla)\n",
    "    if not isinstance(task_code, int):\n",
    "        raise TypeError(f\"run_decoder esperaba task_code como int, pero recibió {type(task_code)}\")\n",
    "\n",
    "    # --- Inicialización de la Secuencia de Tokens (como en el original) ---\n",
    "    eot_token_id = 50257      # <|endoftext|>\n",
    "    sot_token_id = 50258      # <|startoftranscript|>\n",
    "    # task_code                # Idioma/Tarea (ej. 50262 para <|es|>)\n",
    "    token_50359 = 50359        # Token desconocido o específico del modelo (¿quizás <|transcribe|>?)\n",
    "    no_timestamps_id = 50363 # <|notimestamps|>\n",
    "    timestamp_begin = 50364    # Inicio de tokens de tiempo\n",
    "\n",
    "    # Secuencia inicial [sot, lang, ???, no_timestamps]\n",
    "    initial_tokens = [sot_token_id, task_code, token_50359, no_timestamps_id]\n",
    "    tokens = list(initial_tokens) # Copia modificable\n",
    "\n",
    "    # --- Lógica Específica de Padding/Pop del script original ---\n",
    "    # El decoder parece esperar una longitud fija de entrada (12 en el original)\n",
    "    max_tokens_input_len = 12\n",
    "    pop_id = max_tokens_input_len # Índice desde donde se empieza a eliminar/reemplazar\n",
    "\n",
    "    # Rellenar/Inicializar la lista `tokens` a longitud `max_tokens_input_len`\n",
    "    # repitiendo la secuencia inicial. (Confirmado como causa del error si task_code no es int)\n",
    "    if len(tokens) < max_tokens_input_len:\n",
    "         num_repeats = max_tokens_input_len // len(initial_tokens)\n",
    "         remainder = max_tokens_input_len % len(initial_tokens)\n",
    "         # Construye la lista asegurando que todos los elementos sean enteros\n",
    "         tokens = [int(t) for t in initial_tokens] * num_repeats + [int(t) for t in initial_tokens[:remainder]]\n",
    "         print(f\"Advertencia: Se inicializaron los tokens a una longitud fija de {max_tokens_input_len} \"\n",
    "               f\"repitiendo la secuencia inicial. Esto es específico de este script.\")\n",
    "    elif len(tokens) > max_tokens_input_len:\n",
    "         tokens = [int(t) for t in tokens[:max_tokens_input_len]] # Truncar si es más larga\n",
    "         print(f\"Advertencia: Se truncaron los tokens iniciales a {max_tokens_input_len}.\")\n",
    "    else:\n",
    "        # Asegura que todos sean enteros incluso si la longitud era correcta\n",
    "        tokens = [int(t) for t in tokens]\n",
    "\n",
    "    # Verifica que la lista 'tokens' ahora solo contenga enteros\n",
    "    if not all(isinstance(t, int) for t in tokens):\n",
    "        raise TypeError(f\"La inicialización de tokens falló, todavía contiene no enteros: {tokens}\")\n",
    "\n",
    "    # Almacenar el texto generado\n",
    "    generated_text = \"\"\n",
    "    max_decoding_steps = 224 # Límite de seguridad\n",
    "    print(f\"\\nIniciando decodificación (máx {max_decoding_steps} pasos):\")\n",
    "    print(f\"  Tokens iniciales (len={len(tokens)}): {tokens}\") # Ahora deberían ser todos ints\n",
    "\n",
    "    next_token_id = -1 # Inicializa para el bucle while\n",
    "\n",
    "    # Bucle principal de decodificación\n",
    "    for step in range(max_decoding_steps):\n",
    "        # 1. Ejecutar un paso del decoder usando la función _decode (revertida)\n",
    "        try:\n",
    "            # Pasamos la LISTA 'tokens', _decode la convertirá a array internamente\n",
    "            logits = _decode(decoder_model, tokens, out_encoder)\n",
    "            # Logits tiene forma (1, seq_len, vocab_size)\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError durante el paso de decodificación {step+1} en _decode: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            break # Salir del bucle\n",
    "\n",
    "        # 2. Seleccionar el siguiente token (Greedy Search)\n",
    "        # El logit para el *siguiente* token está en la última posición temporal.\n",
    "        next_token_logits = logits[0, -1, :] # Shape: (vocab_size,)\n",
    "        next_token_id = next_token_logits.argmax()\n",
    "\n",
    "        # 3. Convertir ID a token string y manejar Mojibake\n",
    "        if str(next_token_id) in vocab:\n",
    "            raw_token_str = vocab[str(next_token_id)]\n",
    "            processed_token_str = raw_token_str\n",
    "            # ---- Intento de Corrección de Mojibake ----\n",
    "            try:\n",
    "                starts_with_G = raw_token_str.startswith('\\u0120')\n",
    "                core_token_str = raw_token_str[1:] if starts_with_G else raw_token_str\n",
    "                original_bytes = core_token_str.encode('latin-1')\n",
    "                corrected_core = original_bytes.decode('utf-8')\n",
    "                processed_token_str = ('\\u0120' + corrected_core) if starts_with_G else corrected_core\n",
    "                # if processed_token_str != raw_token_str:\n",
    "                #      print(f\"    (Corrección Mojibake: '{raw_token_str}' -> '{processed_token_str}')\")\n",
    "            except (UnicodeEncodeError, UnicodeDecodeError):\n",
    "                 processed_token_str = raw_token_str # Usa el original si falla la corrección\n",
    "            # ---- Fin Corrección Mojibake ----\n",
    "            next_token_str = processed_token_str\n",
    "        else:\n",
    "             print(f\"Advertencia: ID de token {next_token_id} no encontrado en el vocabulario.\")\n",
    "             next_token_str = f\"<UNK_{next_token_id}>\"\n",
    "\n",
    "        print(f\"  Paso {step+1}: Predicho ID={next_token_id}, Token='{next_token_str}'\")\n",
    "\n",
    "        # 4. Comprobar condición de parada (<|eot|>)\n",
    "        if next_token_id == eot_token_id:\n",
    "            print(f\"  Token de fin de texto ({eot_token_id}) detectado. Terminando decodificación.\")\n",
    "            # No añadir <|eot|> a la secuencia 'tokens' final ni al texto.\n",
    "            break\n",
    "\n",
    "        # 5. Añadir el nuevo token ID a la secuencia `tokens`\n",
    "        tokens.append(next_token_id) # Añade al final\n",
    "\n",
    "        # 6. Saltar la adición al texto si es un timestamp\n",
    "        if next_token_id >= timestamp_begin:\n",
    "            print(f\"    (Token de timestamp {next_token_id}, omitido del texto)\")\n",
    "            # PERO AÚN SE ACTUALIZA LA LISTA DE TOKENS con el pop siguiente\n",
    "\n",
    "        # 7. Actualizar la lista `tokens` usando la lógica `pop_id` del original\n",
    "        #    Esto mantiene la longitud de `tokens` fija en `max_tokens_input_len`.\n",
    "        if pop_id > len(initial_tokens): # Si pop_id > 4\n",
    "            pop_id -= 1\n",
    "\n",
    "        # Elimina el token en la posición `pop_id` (ahora que ya se añadió el nuevo al final)\n",
    "        try:\n",
    "            removed_token = tokens.pop(pop_id)\n",
    "            # print(f\"    Token {removed_token} eliminado de índice {pop_id}. pop_id ahora {pop_id if pop_id <= len(initial_tokens) else pop_id -1}. Len tokens: {len(tokens)}\")\n",
    "        except IndexError:\n",
    "            print(f\"Error: Índice pop_id={pop_id} fuera de rango para tokens (len={len(tokens)}). Deteniendo.\")\n",
    "            break\n",
    "\n",
    "\n",
    "        # 8. Añadir el token string al resultado (solo si no es timestamp)\n",
    "        if next_token_id < timestamp_begin:\n",
    "             generated_text += next_token_str\n",
    "             # print(f\"    Texto acumulado: '{generated_text}'\")\n",
    "\n",
    "    else: # Se ejecuta si el bucle for termina sin 'break' (límite alcanzado)\n",
    "        print(f\"\\nAdvertencia: Se alcanzó el límite máximo de pasos de decodificación ({max_decoding_steps}).\")\n",
    "\n",
    "    # --- Post-procesamiento del Texto ---\n",
    "    # Reemplaza 'Ġ' por espacio, limpia tokens especiales residuales.\n",
    "    final_text = generated_text.replace('\\u0120', ' ').replace('<|endoftext|>', '').replace('\\n', '').strip()\n",
    "\n",
    "    # Decodificación Base64 (si es Chino - task_code 50260)\n",
    "    if task_code == 50260:\n",
    "        print(\"Tarea detectada como Chino (ZH), intentando decodificación Base64...\")\n",
    "        try:\n",
    "            missing_padding = len(final_text) % 4\n",
    "            if missing_padding: final_text += '=' * (4 - missing_padding)\n",
    "            final_text = base64.b64decode(final_text).decode('utf-8')\n",
    "            print(\"Decodificación Base64 completada.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error durante la decodificación Base64: {e}. Devolviendo texto como está.\")\n",
    "\n",
    "    return final_text\n",
    "\n",
    "\n",
    "# --- Funciones de Inicialización/Liberación del Modelo ---\n",
    "\n",
    "def init_model(model_path, target=None, device_id=None):\n",
    "    \"\"\"\n",
    "    Inicializa y carga un modelo ONNX o RKNN.\n",
    "    \"\"\"\n",
    "    print(f\"Inicializando modelo desde: {model_path}\")\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"Archivo de modelo no encontrado: {model_path}\")\n",
    "\n",
    "    if model_path.endswith(\".rknn\"):\n",
    "        print(\"Detectado modelo RKNN.\")\n",
    "        model = RKNN(verbose=False)\n",
    "        print('--> Cargando modelo RKNN...')\n",
    "        ret = model.load_rknn(model_path)\n",
    "        if ret != 0:\n",
    "            raise RuntimeError(f\"Fallo al cargar RKNN {model_path}: {ret}\")\n",
    "        print('    Modelo RKNN cargado con éxito.')\n",
    "        print(f'--> Inicializando entorno de ejecución RKNN (Target: {target}, Device ID: {device_id})...')\n",
    "        ret = model.init_runtime(target=target, device_id=device_id)\n",
    "        if ret != 0:\n",
    "            model.release()\n",
    "            raise RuntimeError(f\"Fallo al inicializar runtime RKNN: {ret}\")\n",
    "        print('    Entorno de ejecución RKNN inicializado con éxito.')\n",
    "        return model\n",
    "\n",
    "    elif model_path.endswith(\".onnx\"):\n",
    "        print(\"Detectado modelo ONNX.\")\n",
    "        print('--> Creando sesión de inferencia ONNX Runtime...')\n",
    "        try:\n",
    "            providers = ['CPUExecutionProvider'] # Opcionalmente añadir 'CUDAExecutionProvider' si hay GPU\n",
    "            print(f\"    Usando proveedores: {providers}\")\n",
    "            model = onnxruntime.InferenceSession(model_path, providers=providers)\n",
    "            print(f\"    Sesión ONNX creada. Proveedor activo: {model.get_providers()}\")\n",
    "            return model\n",
    "        except Exception as e:\n",
    "            print(f\"Error al crear la sesión ONNX para {model_path}: {e}\")\n",
    "            raise\n",
    "    else:\n",
    "        raise ValueError(f\"Formato de modelo no soportado: {model_path}. Use .rknn o .onnx\")\n",
    "\n",
    "\n",
    "def release_model(model):\n",
    "    \"\"\"\n",
    "    Libera los recursos asociados a un modelo cargado (RKNN o ONNX).\n",
    "    \"\"\"\n",
    "    if model is None: return\n",
    "    if isinstance(model, RKNN):\n",
    "        print(\"Liberando recursos del modelo RKNN...\")\n",
    "        model.release()\n",
    "        print(\"Modelo RKNN liberado.\")\n",
    "    elif isinstance(model, onnxruntime.InferenceSession):\n",
    "        print(\"Liberando modelo ONNX (implícito al eliminar referencia)...\")\n",
    "        del model\n",
    "        print(\"Referencia a sesión ONNX eliminada.\")\n",
    "    else:\n",
    "        print(f\"Tipo de modelo desconocido ({type(model)}), no se puede liberar explícitamente.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26f51d0",
   "metadata": {},
   "source": [
    "Ejecución del la inferencia usando modelo en ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74d7ffa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "=== Iniciando Parte 3: Inferencia Local ===\n",
      "============================================================\n",
      "Archivo de Audio: ./test_es.wav\n",
      "\n",
      "--- Configuración de Transcripción ---\n",
      "  Encoder: ./model/whisper_encoder_rjac_whisper-tiny-spanish.onnx\n",
      "  Decoder: ./model/whisper_decoder_rjac_whisper-tiny-spanish.onnx\n",
      "  Filters: ./model/mel_80_filters.txt\n",
      "  Vocab:   ./model/vocab_es.txt\n",
      "  Target NPU: ('rk3588',) (Device ID: (None,))\n",
      "[1/4] Cargando vocabulario...\n",
      "Leyendo vocabulario desde: ./model/vocab_es.txt...\n",
      "Vocabulario leído. 51865 tokens cargados.\n",
      "\n",
      "[2/4] Cargando y preprocesando audio: ./test_es.wav...\n",
      "  Audio original - SR: 44100 Hz, Canales: 1, Duración: 9.11s\n",
      "Remuestreando audio: 44100 Hz -> 16000 Hz\n",
      "Remuestreo completado.\n",
      "  Audio preprocesado - SR: 16000 Hz, Canales: 1\n",
      "  Calculando espectrograma Log-Mel...\n",
      "Cargando filtros Mel predeterminados...\n",
      "Filtros Mel ((80, 201)) cargados desde ./model/mel_80_filters.txt\n",
      "Espectrograma Log-Mel calculado. Forma: torch.Size([80, 910])\n",
      "  Ajustando espectrograma a longitud fija 3000...\n",
      "Rellenando espectrograma Mel de 910 a 3000 frames.\n",
      "  Preprocesamiento completado. Forma final Mel: (1, 80, 3000)\n",
      "\n",
      "[3/4] Inicializando modelos Encoder y Decoder...\n",
      "Inicializando modelo desde: ./model/whisper_encoder_rjac_whisper-tiny-spanish.onnx\n",
      "Detectado modelo ONNX.\n",
      "--> Creando sesión de inferencia ONNX Runtime...\n",
      "    Usando proveedores: ['CPUExecutionProvider']\n",
      "    Sesión ONNX creada. Proveedor activo: ['CPUExecutionProvider']\n",
      "Inicializando modelo desde: ./model/whisper_decoder_rjac_whisper-tiny-spanish.onnx\n",
      "Detectado modelo ONNX.\n",
      "--> Creando sesión de inferencia ONNX Runtime...\n",
      "    Usando proveedores: ['CPUExecutionProvider']\n",
      "    Sesión ONNX creada. Proveedor activo: ['CPUExecutionProvider']\n",
      "  Modelos inicializados.\n",
      "\n",
      "[4/4] Ejecutando inferencia...\n",
      "\n",
      "--- Ejecutando Encoder ---\n",
      "Ejecutando inferencia ONNX (Encoder)...\n",
      "  Input ONNX: 'x', Output ONNX: 'out'\n",
      "Salida del Encoder obtenida. Forma: (1, 1500, 384)\n",
      "\n",
      "--- Ejecutando Decoder ---\n",
      "Advertencia: Se inicializaron los tokens a una longitud fija de 12 repitiendo la secuencia inicial. Esto es específico de este script.\n",
      "\n",
      "Iniciando decodificación (máx 224 pasos):\n",
      "  Tokens iniciales (len=12): [50258, 50262, 50359, 50363, 50258, 50262, 50359, 50363, 50258, 50262, 50359, 50363]\n",
      "  Paso 1: Predicho ID=6514, Token='Ġ¡'\n",
      "  Paso 2: Predicho ID=48529, Token='Hola'\n",
      "  Paso 3: Predicho ID=11, Token=','\n",
      "  Paso 4: Predicho ID=5283, Token='Ġesta'\n",
      "  Paso 5: Predicho ID=785, Token='Ġes'\n",
      "  Paso 6: Predicho ID=2002, Token='Ġuna'\n",
      "  Paso 7: Predicho ID=48241, Token='Ġprueba'\n",
      "  Paso 8: Predicho ID=368, Token='Ġde'\n",
      "  Paso 9: Predicho ID=30005, Token='Ġvoz'\n",
      "  Paso 10: Predicho ID=465, Token='Ġen'\n",
      "  Paso 11: Predicho ID=31177, Token='Ġespañol'\n",
      "  Paso 12: Predicho ID=1690, Token='Ġpara'\n",
      "  Paso 13: Predicho ID=6133, Token='Ġevalu'\n",
      "  Paso 14: Predicho ID=289, Token='ar'\n",
      "  Paso 15: Predicho ID=806, Token='Ġel'\n",
      "  Paso 16: Predicho ID=27825, Token='Ġmodelo'\n",
      "  Paso 17: Predicho ID=26018, Token='Ġwhisper'\n",
      "  Paso 18: Predicho ID=257, Token='Ġa'\n",
      "  Paso 19: Predicho ID=24463, Token='Ġtravés'\n",
      "  Paso 20: Predicho ID=368, Token='Ġde'\n",
      "  Paso 21: Predicho ID=15329, Token='ĠPython'\n",
      "  Paso 22: Predicho ID=13, Token='.'\n",
      "  Paso 23: Predicho ID=50257, Token='<|endoftext|>'\n",
      "  Token de fin de texto (50257) detectado. Terminando decodificación.\n",
      "\n",
      "==============================\n",
      "   Transcripción Resultante\n",
      "==============================\n",
      "¡Hola, esta es una prueba de voz en español para evaluar el modelo whisper a través de Python.\n",
      "==============================\n",
      "\n",
      "\n",
      "--- Limpieza de Recursos ---\n",
      "Liberando modelo ONNX (implícito al eliminar referencia)...\n",
      "Referencia a sesión ONNX eliminada.\n",
      "Liberando modelo ONNX (implícito al eliminar referencia)...\n",
      "Referencia a sesión ONNX eliminada.\n",
      "--------------------------\n",
      "\n",
      "Proceso finalizado.\n"
     ]
    }
   ],
   "source": [
    "audio_path = DEFAULT_AUDIO_PATH_FOR_INFERENCE\n",
    "encoder_path = os.path.join(MODEL_SAVE_DIR, f\"whisper_encoder_{DEFAULT_MODEL_NAME.replace('/', '_')}.onnx\")\n",
    "decoder_path = os.path.join(MODEL_SAVE_DIR, f\"whisper_decoder_{DEFAULT_MODEL_NAME.replace('/', '_')}.onnx\")\n",
    "vocab_path = os.path.join(MODEL_SAVE_DIR, \"vocab_es.txt\")\n",
    "filters_path = os.path.join(MODEL_SAVE_DIR, f\"mel_{N_MELS_DEFAULT}_filters.txt\") # Asume N_MELS=80\n",
    "prefer_rknn=False, # Intentar usar RKNN si está disponible\n",
    "rknn_target=DEFAULT_RKNN_PLATFORM,\n",
    "rknn_device_id=None,\n",
    "task_token_id=TASK_CODE_SPANISH\n",
    "max_output_tokens = 100\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"=== Iniciando Parte 3: Inferencia Local ===\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"Archivo de Audio: {audio_path}\")\n",
    "\n",
    "print(\"\\n--- Configuración de Transcripción ---\")\n",
    "print(f\"  Encoder: {encoder_path}\")\n",
    "print(f\"  Decoder: {decoder_path}\")\n",
    "print(f\"  Filters: {filters_path}\")\n",
    "print(f\"  Vocab:   {vocab_path}\")\n",
    "\n",
    "if prefer_rknn:\n",
    "        print(f\"  Target NPU: {rknn_target} (Device ID: {rknn_device_id or 'default'})\")\n",
    "else:\n",
    "        print(\"  Target: CPU (ONNX Runtime)\")\n",
    "        print(\"-------------------------------------\\n\")\n",
    "\n",
    "# Variables para los modelos (inicializadas a None)\n",
    "encoder_model = None\n",
    "decoder_model = None\n",
    "\n",
    "try:\n",
    "        # --- 1. Carga de Vocabulario ---\n",
    "        print(\"[1/4] Cargando vocabulario...\")\n",
    "        vocab = read_vocab(vocab_path)\n",
    "\n",
    "        # --- 2. Carga y Preprocesamiento del Audio ---\n",
    "        print(f\"\\n[2/4] Cargando y preprocesando audio: {audio_path}...\")\n",
    "        audio_data, original_sr = sf.read(audio_path, dtype='float32', always_2d=False)\n",
    "        print(f\"  Audio original - SR: {original_sr} Hz, Canales: {audio_data.ndim}, Duración: {len(audio_data)/original_sr:.2f}s\")\n",
    "        audio_data, num_channels = ensure_channels(audio_data, audio_data.ndim)\n",
    "        if num_channels != 1: raise ValueError(\"Se requiere audio mono.\")\n",
    "        audio_data, current_sr = ensure_sample_rate(audio_data, original_sr)\n",
    "        if current_sr != SAMPLE_RATE: raise ValueError(\"Fallo en remuestreo.\")\n",
    "        print(f\"  Audio preprocesado - SR: {current_sr} Hz, Canales: 1\")\n",
    "\n",
    "        print(\"  Calculando espectrograma Log-Mel...\")\n",
    "        log_mel_features = log_mel_spectrogram(audio_data, filters_path=filters_path)\n",
    "        log_mel_numpy = log_mel_features.numpy()\n",
    "\n",
    "        print(f\"  Ajustando espectrograma a longitud fija {MAX_LENGTH}...\")\n",
    "        x_mel = pad_or_trim(log_mel_numpy, length=MAX_LENGTH)\n",
    "        x_mel = np.expand_dims(x_mel, 0) # Añadir Batch dim -> (1, N_MELS, MAX_LENGTH)\n",
    "        print(f\"  Preprocesamiento completado. Forma final Mel: {x_mel.shape}\")\n",
    "\n",
    "        # --- 3. Inicialización de Modelos ---\n",
    "        print(\"\\n[3/4] Inicializando modelos Encoder y Decoder...\")\n",
    "        encoder_model = init_model(encoder_path, rknn_target, rknn_device_id)\n",
    "        decoder_model = init_model(decoder_path, rknn_target, rknn_device_id)\n",
    "        print(\"  Modelos inicializados.\")\n",
    "\n",
    "        # --- 4. Inferencia ---\n",
    "        print(\"\\n[4/4] Ejecutando inferencia...\")\n",
    "        print(\"\\n--- Ejecutando Encoder ---\")\n",
    "        out_encoder = run_encoder(encoder_model, x_mel)\n",
    "\n",
    "        print(\"\\n--- Ejecutando Decoder ---\")\n",
    "        # Pasar el task_token_id asegurado como entero\n",
    "        result = run_decoder(decoder_model, out_encoder, vocab, task_token_id)\n",
    "\n",
    "        # --- Mostrar Resultado ---\n",
    "        print(\"\\n\" + \"=\"*30)\n",
    "        print(\"   Transcripción Resultante\")\n",
    "        print(\"=\"*30)\n",
    "        print(result)\n",
    "        print(\"=\"*30 + \"\\n\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "         print(f\"\\nError Fatal: Archivo no encontrado - {e}\")\n",
    "         exit(1)\n",
    "except (ValueError, TypeError) as e: # Captura ambos tipos de error comunes\n",
    "         print(f\"\\nError Fatal: Problema con los datos o configuración - {e}\")\n",
    "         import traceback\n",
    "         traceback.print_exc()\n",
    "         exit(1)\n",
    "except RuntimeError as e:\n",
    "         print(f\"\\nError Fatal: Problema con el runtime (RKNN/ONNX) - {e}\")\n",
    "         exit(1)\n",
    "except Exception as e:\n",
    "        print(f\"\\nError Inesperado: {e}\")\n",
    "        import traceback\n",
    "        print(\"\\n--- Traceback ---\"); traceback.print_exc(); print(\"-----------------\\n\")\n",
    "        exit(1)\n",
    "finally:\n",
    "        # --- Liberar Recursos ---\n",
    "        print(\"\\n--- Limpieza de Recursos ---\")\n",
    "        release_model(encoder_model)\n",
    "        release_model(decoder_model)\n",
    "        print(\"--------------------------\\n\")\n",
    "        print(\"Proceso finalizado.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rknn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
